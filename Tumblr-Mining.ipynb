{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#===================================================\n",
    "# - Takes the pulled data and preprocesses it\n",
    "# - Creates a Skip-Gram model of vector lengths 100\n",
    "# - Saves our model to .bin files\n",
    "# - Saves various text data to pandas data frames\n",
    "#===================================================\n",
    "\n",
    "import pytumblr     # Tumblr API library\n",
    "import json         # JSON module: Converts python dictionaries into strings\n",
    "import codecs       #\n",
    "import re           # Regular Expression library\n",
    "import nltk         # (Natural Language Processing Tool Kit) tokenizes sentences for me\n",
    "import gensim       # Library for Word Embedding / Word2Vec\n",
    "import pandas as pd # Used to create data frames with rows and columns\n",
    "import numpy as np  # \n",
    "import matplotlib   # PLotting library\n",
    "import imblearn     # Our testing sample is umbalanced, methods to create dummy samples\n",
    "import sklearn      # More Machine Learning Algorithms\n",
    "import warnings     # Lets me ignore warning messages\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "\n",
    "# App's credentials to use Tumblr API library\n",
    "# (tokens and secrets are considered sensitive information; thus, my tokens are not actually shown here)\n",
    "CONSUMER_KEY = '**************************************************'\n",
    "CONSUMER_SECRET = '**************************************************'\n",
    "OAUTH_TOKEN = '**************************************************'\n",
    "OAUTH_SECRET = '**************************************************'\n",
    "\n",
    "# Authentictates the app using Tumblr credentials\n",
    "client = pytumblr.TumblrRestClient(\n",
    "    CONSUMER_KEY,\n",
    "    CONSUMER_SECRET,\n",
    "    OAUTH_TOKEN,\n",
    "    OAUTH_SECRET,\n",
    ")\n",
    "\n",
    "\n",
    "# Removes warning messages from output\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "# Helps print JSON or lists that are all jumbled together (too hard to read)\n",
    "def print2(o):\n",
    "    print(json.dumps(o, indent=1))\n",
    "\n",
    "# Print options for data frames\n",
    "pd.set_option('display.max_rows', None)       # Prints all rows\n",
    "#pd.set_option('display.max_columns', None)    # Prints all columns\n",
    "pd.set_option('display.width', None)          # Doesn't allow the text to wrap around underneath the bottom row\n",
    "pd.set_option('display.max_colwidth', -1)     # Prints an entire sentence/paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "\n",
    "#****\n",
    "# Pulls the content from posts and records it to a list (does not pull images, only text)\n",
    "# \"blog_post\" is a list-of-dictionaries, each dictionary is a post; 'blog_post' is created using tagSearch()\n",
    "#\n",
    "# %% Returns A list of strings (sentences)\n",
    "#****\n",
    "\n",
    "def pullPost(blog_post):\n",
    "    # Lists for following keys: summary, content_raw, comment\n",
    "    raw_content = []\n",
    "    # Temporary list, used to hold a 'dictionary' that has the required content inside in a specific 'key'\n",
    "    temp = []\n",
    "    \n",
    "    # Add a blog post's \"summary\" to the list\n",
    "    for x in blog_post:\n",
    "        try:\n",
    "            raw_content.append (x['summary'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add a blog post's \"content_raw\" to the list\n",
    "    for x in blog_post:\n",
    "        try:\n",
    "            temp.append (x['trail'])\n",
    "            for x in temp:\n",
    "                for y in x:\n",
    "                    raw_content.append (y['content_raw'])\n",
    "        except:\n",
    "            pass\n",
    "    temp.clear()\n",
    "    \n",
    "    # A list-of-strings (sentences)\n",
    "    return raw_content\n",
    "\n",
    "#========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "\n",
    "#****\n",
    "# - Takes raw data and PreProcesses it.\n",
    "#   Cleans unwanted text and each sentence is represented as its own list-of-words\n",
    "# - 'list_stringsX' is a list-of-strings (sentences)\n",
    "#\n",
    "# %% Returns a list-of-lists; each inner-list represents a single split-sentence\n",
    "#****\n",
    "def preProc(list_stringsX):\n",
    "    \n",
    "    # Copies the input list to a temporary list; so that we don't overwrite the input list\n",
    "    list_strings = list_stringsX.copy()\n",
    "    \n",
    "    # Removes un-needed text\n",
    "    # \"list_strings\" is a list of strings\n",
    "    for x in range(len(list_strings)):\n",
    "        list_strings[x] = list_strings[x].lower()                        # Lowercases all alpha-characters\n",
    "        list_strings[x] = re.sub(r'http\\S+', ' ', list_strings[x])       # Removes strings that start with http\n",
    "        list_strings[x] = re.sub(r'https\\S+', ' ', list_strings[x])      # Removes strings that start with https\n",
    "        list_strings[x] = re.sub(r\"'\", '', list_strings[x])              # Removes apostrophes\n",
    "        list_strings[x] = re.sub(r\"â€™\", '', list_strings[x])              # Removes apostrophes in a different font\n",
    "        list_strings[x] = re.sub(r\",\", '', list_strings[x])              # Removes commas\n",
    "        list_strings[x] = re.sub(r'@\\w+', '', list_strings[x])           # Removes string prefixed '@'\n",
    "        list_strings[x] = re.sub(r'\\s#\\w+', '', list_strings[x])         # Removes'#' and anything connected to it\n",
    "        list_strings[x] = re.sub(r'\\W',' ',list_strings[x])              # Removes other special characters\n",
    "        list_strings[x] = re.sub(r'\\s*\\d\\s*', '', list_strings[x])       # Removes numbers/digits\n",
    "        list_strings[x] = re.sub(r'\\^[a-zA-Z]\\s+','', list_strings[x])   # Removes single char from beginning\n",
    "        list_strings[x] = re.sub(r'^b\\s+','', list_strings[x])           # Removes prefixed 'b'\n",
    "        list_strings[x] = re.sub(r'((?<=^)|(?<= )).((?=$)|(?= ))', '', list_strings[x])  # Removes single chars\n",
    "        list_strings[x] = re.sub(r'\\s+',' ',list_strings[x])             # Replace multiple spaces with one space\n",
    "        \n",
    "        \n",
    "    #====\n",
    "    # A list of pre-processed sentences, each list has only 1 item: a whole sentence\n",
    "    #print(list_strings)\n",
    "    #====\n",
    "\n",
    "\n",
    "    # A list to hold each sentence list\n",
    "    list_sentence = []\n",
    "\n",
    "    # Splits all sentences into individual words\n",
    "    for x in list_strings:    \n",
    "        list_sentence.append (x.split())\n",
    "\n",
    "    #====\n",
    "    # A list-of-lists, each list is comprised of each word of its respective sentence\n",
    "    #print(list_sentence)\n",
    "    #====\n",
    "    \n",
    "    # Returns a list-of-lists\n",
    "    return list_sentence\n",
    "\n",
    "#========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "\n",
    "#****\n",
    "# - Takes the list-of-words and its corresponding set of\n",
    "#   word-vectors and converts each sentence into a large vector\n",
    "# - 'split_sent' is a list-of-lists (sentence-words)\n",
    "# - 'vectList' is the corresponding Skip Gram vector model to the passed 'split_sent'\n",
    "#\n",
    "# %% Returns a list-of-vectors; each vector represents a single sentence\n",
    "#****\n",
    "def makeMatrix(split_sent, vectList):\n",
    "\n",
    "    # Used to hold all the vectors of a sentence.\n",
    "    # Our template accommodates 20 words;\n",
    "    #  and we'll only use the first 10 elements of each word-vector.\n",
    "    #  So 20 word-vectors, each of length 10, will make a vector of length 200.\n",
    "    #  Sentences with less than 20 words will pad the vector with 0s\n",
    "    templateVector = []\n",
    "\n",
    "    # This will hold all the 200-long sentence-vectors\n",
    "    vectorMatrix = []\n",
    "\n",
    "    # Loops thru each sentence\n",
    "    for sentenceT in range(len(split_sent)):\n",
    "        #print(split_sent[sentenceT]) # each sentence\n",
    "\n",
    "        # Loops thru each sentence's words, adding the first 10 items from each vector to 'templateVector'\n",
    "        for wordT in range(len(split_sent[sentenceT])):\n",
    "            #print(split_sent[sentenceT][wordT])         # each word\n",
    "            #print(model100[split_sent[sentenceT][wordT]]) # each word vector\n",
    "            templateVector.extend(vectList[split_sent[sentenceT][wordT]][0:10])\n",
    "            #print(len(templateVector))\n",
    "\n",
    "        # Pads with 0.0s if less than 200; or slices the list to exactly 200\n",
    "        templateVector = templateVector[:200] + [0.0]*(200 - len(templateVector))\n",
    "        #print(len(templateVector))\n",
    "\n",
    "        # Add the sentence vector to the big list\n",
    "        vectorMatrix.append(templateVector)\n",
    "        #print(len(vectorMatrix[sentenceT]))\n",
    "\n",
    "        # Reset templateVector for the next sentence\n",
    "        templateVector = []\n",
    "\n",
    "\n",
    "        # Tests the first few sentences\n",
    "        #if(sentenceT == 2):\n",
    "        #    break\n",
    "        \n",
    "    return vectorMatrix\n",
    "\n",
    "#========================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# We're borrowing a set of predefined data from a GitHub repository\n",
    "# It will be used to train our sentiment prediction model\n",
    "#****\n",
    "\n",
    "# Refers to Airline tweets from GitHub\n",
    "# 'GH_data_frame' is a pandas data frame\n",
    "AirlineData = \"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\"\n",
    "GH_data_frame = pd.read_csv(AirlineData)\n",
    "\n",
    "# Assigns the tweets and sentiments to their own variable, we don't need the other data from the data frame\n",
    "Atweets = GH_data_frame.iloc[:, 10].values\n",
    "Asentiment = GH_data_frame.iloc[:, 1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Saves more recent data to \"TumData3.txt\"; \"TumData\" and \"TumData2\" has old news\n",
    "#****\n",
    "\n",
    "# The current text data file \n",
    "file3 = codecs.open('./txtFiles/TumData3.txt', 'w+', \"utf-8\")\n",
    "\n",
    "# Holds the textual content of the pulled posts\n",
    "currentText = []\n",
    "\n",
    "# Initiates a search in Tumblr, finding the top 20 text-based blog posts for the specified topic\n",
    "currentText.extend(pullPost(client.tagged('Macintosh', filter = 'text')))\n",
    "\n",
    "# Overwrites old posts with newly pulled posts\n",
    "for line in currentText:\n",
    "    file3.write(line)\n",
    "    \n",
    "file3.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Takes data from myself and my other 2 teammates and PreProcesses them, then combines them all together\n",
    "# Keeps each post intact by saving each one as its own list-of-words\n",
    "#****\n",
    "\n",
    "# This will hold all of our data as a list-of-lists; list-of-split-sentences (split-sentences = list-of-words)\n",
    "all_data1 = [] # Before removing trivial words (Used to train the Skip-Gram model)\n",
    "all_data2 = [] # After removing trivial words  (Used to train the RandomForest Classifier model)\n",
    "\n",
    "# This will only hold the data from the social networks\n",
    "our_data1 = [] # Before removing trivial words\n",
    "our_data2 = [] # After removing trivial words\n",
    "\n",
    "# Opens every .txt file\n",
    "twit = open(\"./txtFiles/TweetData.txt\", \"r\", encoding=\"utf8\")\n",
    "redd = open(\"./txtFiles/RedData.txt\", \"r\", encoding=\"utf8\")\n",
    "tum  = open(\"./txtFiles/TumData.txt\", \"r\", encoding=\"utf8\")\n",
    "tum2 = open(\"./txtFiles/TumData2.txt\", \"r\", encoding=\"utf8\")\n",
    "tum3 = open(\"./txtFiles/TumData3.txt\", \"r\", encoding=\"utf8\")\n",
    "tum4 = open(\"./txtFiles/TumData4.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "# \"Ignores\" newlines\n",
    "twit_txt = [x for x in twit.readlines() if x.strip()]\n",
    "redd_txt = [x for x in redd.readlines() if x.strip()]\n",
    "tum_txt  = [x for x in tum.readlines() if x.strip()]\n",
    "tum2_txt = [x for x in tum2.readlines() if x.strip()]\n",
    "tum3_txt = [x for x in tum3.readlines() if x.strip()]\n",
    "tum4_txt = [x for x in tum4.readlines() if x.strip()]\n",
    "\n",
    "\n",
    "# Takes Twitter text file and PreProcesses the data (provided by my teammate)\n",
    "tweet_data = preProc(twit_txt)\n",
    "\n",
    "# Takes Reddit text file and PreProcesses the data (provided by my other teammate)\n",
    "red_data = preProc(redd_txt)\n",
    "\n",
    "# Takes my first text file and PreProcesses the data\n",
    "tum_data = preProc(tum_txt)\n",
    "\n",
    "# Takes my second text file and PreProcesses the data\n",
    "tum_data2 = preProc(tum2_txt)\n",
    "\n",
    "# Takes my third text file and PreProcesses the data\n",
    "tum_data3 = preProc(tum3_txt)\n",
    "\n",
    "# ...\n",
    "tum_data4 = preProc(tum4_txt)\n",
    "\n",
    "\n",
    "# PreProcesses the GitHub's airline data\n",
    "airline_data1 = preProc(Atweets)\n",
    "airline_data2 = []\n",
    "\n",
    "\n",
    "# Combines all our data (including the data from GitHub) for machine learning\n",
    "all_data1 = airline_data1 + tweet_data + red_data + tum_data + tum_data2 + tum_data3 + tum_data4\n",
    "\n",
    "# Combines all our data (excluding the data from GitHub) for sentiment analysis\n",
    "our_data1 = tweet_data + red_data + tum_data + tum_data2 + tum_data3 + tum_data4\n",
    "\n",
    "# Removes '\\n'\n",
    "all_data1[:] = [x for x in all_data1 if x != '\\n']\n",
    "our_data1[:] = [x for x in our_data1 if x != '\\n']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Takes the extracted data and removes \"stop-words\"/trivial words from the textual data\n",
    "#****\n",
    "\n",
    "# List of trivial-words or \"stop-words\": prepositions, pronouns, articles, anything with little contextual meaning\n",
    "stopWords = ['to','and', 'is', 'a', 'of', 'on', 'the', 'for', 'he','did','was',\n",
    "             'in', 'with', 'about', 'this', 'are','from','his','that','have',\n",
    "             'they', 'by','has', 'so', 'be', 'will', 'do','also','it', 'but',\n",
    "             'been','if', 'but', 'you', '...', 'an', 'as', 'at', 'comment',\n",
    "             'your', 'like', 'what', 'i', 'my','com']\n",
    "\n",
    "# Removes \"stop-words\" from all_data\n",
    "for item in all_data1:\n",
    "    all_data2.append([x for x in item if x not in stopWords])\n",
    "\n",
    "# Removes \"stop-words\" from our_data\n",
    "for item in our_data1:\n",
    "    our_data2.append([x for x in item if x not in stopWords])\n",
    "    \n",
    "# Removes \"stop-words\" from airline_data\n",
    "for item in airline_data1:\n",
    "    airline_data2.append([x for x in item if x not in stopWords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#============================================================================\n",
    "# min_count = Ignores all words with total frequency lower than value\n",
    "#      size = length of each word's vector\n",
    "#    window = represents the distance of neighboring words to train the model\n",
    "#        sg = training algorithm: CBOW (Default) or Skip Gram (1)\n",
    "#============================================================================\n",
    "\n",
    "# Skip Gram model, word vectors have a length of 100\n",
    "model100 = gensim.models.Word2Vec(all_data1, min_count = 1, size = 100, window = 5, sg = 1)\n",
    "\n",
    "# Saves Word2Vec object (\"model100\") to a .bin file (to be distributed elsewhere)\n",
    "model100.save('model100.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Saves all our textual data to different data frames for later usage\n",
    "#****\n",
    "\n",
    "# Holds all the unprocessed text data\n",
    "posts_data = []\n",
    "\n",
    "# Opens the files again\n",
    "twit = open(\"./txtFiles/TweetData.txt\", \"r\", encoding=\"utf8\")\n",
    "redd = open(\"./txtFiles/RedData.txt\", \"r\", encoding=\"utf8\")\n",
    "tum  = open(\"./txtFiles/TumData.txt\", \"r\", encoding=\"utf8\")\n",
    "tum2 = open(\"./txtFiles/TumData2.txt\", \"r\", encoding=\"utf8\")\n",
    "tum3 = open(\"./txtFiles/TumData3.txt\", \"r\", encoding=\"utf8\")\n",
    "tum4 = open(\"./txtFiles/TumData4.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "# Ignores newlines\n",
    "twit_txt = [x for x in twit.readlines() if x.strip()]\n",
    "redd_txt = [x for x in redd.readlines() if x.strip()]\n",
    "tum_txt  = [x for x in tum.readlines() if x.strip()]\n",
    "tum2_txt = [x for x in tum2.readlines() if x.strip()]\n",
    "tum3_txt = [x for x in tum3.readlines() if x.strip()]\n",
    "tum4_txt = [x for x in tum4.readlines() if x.strip()]\n",
    "\n",
    "\n",
    "# Appends all unprocessed text to the 'posts_data' list\n",
    "posts_data = twit_txt\n",
    "posts_data.extend(redd_txt)\n",
    "posts_data.extend(tum_txt)\n",
    "posts_data.extend(tum2_txt)\n",
    "posts_data.extend(tum3_txt)\n",
    "posts_data.extend(tum4_txt)\n",
    "\n",
    "# Removes '\\n'\n",
    "posts_data[:] = [x for x in posts_data if x != '\\n']\n",
    "for x in range(len(posts_data)):\n",
    "    posts_data[x] = re.sub(r'\\n', ' ', posts_data[x])\n",
    "    posts_data[x] = re.sub(r'\\s+',' ',posts_data[x])\n",
    "\n",
    "# Saves all our unprocessed post data to a data frame (in reverse order)\n",
    "our_og_df = pd.DataFrame(list(zip(posts_data[::-1])), columns=['Our_posts'])\n",
    "\n",
    "# Saves all our preprocessed data to a data frame (in reverse order)\n",
    "our_pre_df = pd.DataFrame(list(zip(our_data2[::-1])), columns=['Our_Preprocess'])\n",
    "\n",
    "# Saves the usable GitHub data to a data frame (split and sentiments)\n",
    "small_air_df = pd.DataFrame(list(zip(airline_data2, Asentiment)), columns=['Tweet', 'Sentiment'])\n",
    "\n",
    "# We'll also save the original posts from the airline GitHub data (just the tweets)\n",
    "og_air_df = pd.DataFrame(list(zip(GH_data_frame.iloc[:, 10].values)), columns=['Github_posts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Saves the processed and unprocessed texts to their own files\n",
    "#****\n",
    "\n",
    "our_pre_df.to_csv(('our_pre.csv'),index=False)\n",
    "our_og_df.to_csv(('our_og.csv'),index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=====================================================================================\n",
    "# - Takes all of our data and converts each sentence into their vector representations\n",
    "#=====================================================================================\n",
    "\n",
    "small_air_posts = small_air_df.iloc[:,0].values # Split airline posts (these are arrays, not lists)\n",
    "sentiment = small_air_df.iloc[:,1].values       # Airline sentiments\n",
    "\n",
    "our_pre_posts = our_pre_df.iloc[:,0].values # Our split posts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Calls makeMatrix() to convert 'small_air_posts', and 'our_pre_posts' into their vector representations  \n",
    "#****\n",
    "\n",
    "# Creates vector matrices for the two data sets\n",
    "testMatrix = makeMatrix(small_air_posts, model100) # Used for training\n",
    "realMatrix = makeMatrix(our_pre_posts, model100)   # Used for evaluating our social network data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Converts the matrices into data frames\n",
    "#****\n",
    "\n",
    "testMatrix_df = pd.DataFrame(list(zip(testMatrix, sentiment)), columns=['Air_vectors', 'Sentiment'])\n",
    "realMatrix_df = pd.DataFrame(zip(realMatrix), columns=['Our_vectors'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#==============================================================================================\n",
    "# Takes the vector matrices from the previous step and uses them to train our prediction model\n",
    "# Then it will use the trained model to predict the sentiments of our data sets\n",
    "#==============================================================================================\n",
    "\n",
    "sentiment = testMatrix_df.iloc[:,1].values.tolist()  # Sentiments, and keeps them in a list\n",
    "\n",
    "our_og_posts = our_og_df.iloc[:,0].values.tolist() # The original social network posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converts the sentiment Strings into integers for sklearn\n",
    "for i, item in enumerate(sentiment):\n",
    "    if(item == 'negative'):\n",
    "        sentiment[i] = 0\n",
    "    elif(item == 'neutral'):\n",
    "        sentiment[i] = 1\n",
    "    elif(item == 'positive'):\n",
    "        sentiment[i] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# We have too many negative samples, so this creates dummies of positive and neutral samples\n",
    "#****\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "testMatrix, sentiment = smote.fit_sample(testMatrix, sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Allocate 80% of airline data to train the classifier, and 20% for testing\n",
    "# Also, set forth some parameters of the training model \n",
    "#****\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "# Vect_train : train using 80% of data sample\n",
    "# Vect_test  : test predictions using other 20% of data sample\n",
    "# Sent_train : label for Vect_train\n",
    "# Sent_test  : label for Vect_test\n",
    "#--------------------------------------------------------------\n",
    "Vect_train, Vect_test, Sent_train, Sent_test = train_test_split(testMatrix, sentiment, test_size=0.2, random_state=0)\n",
    "\n",
    "# Parameter settings for the prediction/classifier model\n",
    "text_classifier_test = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "#****\n",
    "# - RandomForest creates decision trees on randomly selected data samples, then\n",
    "#   gets prediction from each tree and selects the best solution.\n",
    "# - That's what the documentation says at least, not sure what it means exactly though\n",
    "#****\n",
    "\n",
    "# Training set \n",
    "text_classifier_test.fit(Vect_train, Sent_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Make predictions on the airline test set\n",
    "# And determine accuracy of prediction model\n",
    "#****\n",
    "\n",
    "# Make predictions on the test set 'Vect_test'\n",
    "testPredict = text_classifier_test.predict(Vect_test)\n",
    "\n",
    "# Compares 'predictions' to 'Sent_test' to see how many were correct\n",
    "# Down hill diagonal represents the correct amount of predictions made\n",
    "cMatrix = confusion_matrix(Sent_test,testPredict)\n",
    "\n",
    "# Converts each value in the matrix into a percentage/float-value\n",
    "cMatrix = cMatrix.astype('float') / cMatrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Convert this list into a numpy array\n",
    "Sent_test = np.asarray(Sent_test, dtype=np.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Print our test results as a confusion matrix\n",
    "#****\n",
    "\n",
    "# \n",
    "boxPlot, ax = plt.subplots()\n",
    "\n",
    "# The tick labels for the box-plot\n",
    "sent_label = ['negative','neutral','positive']\n",
    "\n",
    "# sklearn doesn't like lists, so convert to numpy array\n",
    "sent_label = np.asarray(sent_label, dtype=np.str)\n",
    "\n",
    "# Takes comparison of 'Sent_test' and 'testPredict' and links them to 'sent_label' when we make the box-plot\n",
    "sent_label = sent_label[unique_labels(Sent_test, testPredict)] \n",
    "\n",
    "\n",
    "# (cmap = plt.cm.Greens) represents the colors/shades used for the plot\n",
    "im = ax.imshow(cMatrix, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Configures the labels and tick marks of the box-plot\n",
    "ax.set(xticks=np.arange(cMatrix.shape[1]), # cMatrix.shape[1] = number of columns = x-axis\n",
    "       yticks=np.arange(cMatrix.shape[0]), # cMatrix.shape[0] = number of rows = y-axis\n",
    "       \n",
    "       # Label the ticks with their respective sentiment\n",
    "       xticklabels=sent_label, yticklabels=sent_label,\n",
    "       \n",
    "       # Label the sides\n",
    "       title='Confusion Matrix',\n",
    "       ylabel='True Sentiments',\n",
    "       xlabel='Predicted Sentiments')\n",
    "\n",
    "# Rotates the x-axis tick labels to be 45 degrees and properly aligns them with the tick mark\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# cMatrix.max() = largest value in the cMatrix\n",
    "# thresh means threshold; so divide max() by 2 gives puts a threshold at the middle of our values in cMatrix\n",
    "thresh = cMatrix.max() / 2.\n",
    "\n",
    "# Takes data from confusion matrix and puts them in the box-plot\n",
    "for i in range(cMatrix.shape[0]):\n",
    "    \n",
    "    for j in range(cMatrix.shape[1]):\n",
    "        # Places each item of 'cMatrix' into the box-plot\n",
    "        ax.text(j, i, format(cMatrix[i, j], '.2f'),\n",
    "                # Horizontal & Vertical alignment of text is centered\n",
    "                ha=\"center\", va=\"center\",\n",
    "                \n",
    "                # Makes text black for bright background, white for dark background\n",
    "                # Any value higher than 'thresh' becomes white, any value less is black\n",
    "                color=\"white\" if cMatrix[i, j] > thresh else \"black\")\n",
    "\n",
    "# The overall accuracy of our test\n",
    "print('â€¢ Overall accuracy of the prediction/classifier model: ', accuracy_score(Sent_test, testPredict))\n",
    "print()\n",
    "\n",
    "# Prints the confusion matrix\n",
    "boxPlot.tight_layout()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#****\n",
    "# Make predictions on 'realMatrix'; otherwise known as 'Our data'\n",
    "#****\n",
    "\n",
    "# Make predictions on our data set 'realMatrix'\n",
    "realPredict = text_classifier_test.predict(realMatrix)\n",
    "\n",
    "# Convert numpy.array into list, so that we can have mismatching data types\n",
    "realPredict = realPredict.copy().tolist()\n",
    "\n",
    "# Converts these integers into Strings/Sentiments\n",
    "for i, item in enumerate(realPredict):\n",
    "    if(item == 0):\n",
    "        realPredict[i] = 'negative'\n",
    "    elif(item == 1):\n",
    "        realPredict[i] = 'neutral'\n",
    "    elif(item == 2):\n",
    "        realPredict[i] = 'positive'\n",
    "    \n",
    "\n",
    "# Creates a data frame of the original posts and their predicted sentiments\n",
    "ourPredict = pd.DataFrame(list(zip(our_og_posts, realPredict)),columns=['Original Posts', 'Predicted Sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prints our results\n",
    "ourPredict.head(len(ourPredict['Original Posts']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
